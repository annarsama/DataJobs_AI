{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des diff√©rents sites statiques (Emploi territorial & JobsthatmakeSense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emploi territorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.emploi-territorial.fr/emploi-mobilite/?adv-search=data&search-fam-metier=A7\" # Nous r√©cup√©rons simplement la premi√®re page (unique qui renvoie des offres v√©ritablement en relation avec la data)\n",
    "\n",
    "# On cr√©e un dictionnaire pour r√©cup√©rer les informations pertinentes.\n",
    "dict_emploiterr = {\n",
    "    \"entreprise\": [],\n",
    "    \"titre\": [],\n",
    "    \"type_contrat\": [],\n",
    "    \"ville\": [],\n",
    "    \"departement\": [],\n",
    "    \"pays\": [],\n",
    "    \"link\": [],\n",
    "    \"date_publication\": [],\n",
    "    \"descriptif\": [],\n",
    "    \"profil\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On int√©ragit tout d'abord avec la page principale de recherche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    page = requests.get(URL)\n",
    "    # V√©rifie que la requ√™te a √©t√© correctement r√©alis√©e\n",
    "    page.raise_for_status()  \n",
    "\n",
    "    # Cr√©e un objet BeautifulSoup repr√©sentant le document HTML\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    # On extrait la partie de la page qui contient les offres\n",
    "    results = soup.find_all(\"tr\", {\"id\": True})\n",
    "\n",
    "\n",
    "    for job in results:\n",
    "        # On extrait le titre de l'offre d'emploi et on nettoie le texte\n",
    "        title = job.find(\"div\", class_=\"detail-offre detail-offre-titre mb-1\").getText().replace(\"\\n\",\"\").replace(\"(h/f)\",\"\").replace(\"(H/F)\", \"\").strip()\n",
    "        dict_emploiterr[\"titre\"].append(title)\n",
    "        # On extrait le nom de la collectivit√©\n",
    "        entreprise = job.find(\"span\", class_=\"valeur font-weight-bold\")\n",
    "        if entreprise:\n",
    "            entreprise = entreprise.getText().replace(\"\\n\",\"\").strip()\n",
    "        else:\n",
    "            entreprise = \"Non sp√©cifi√©\"\n",
    "        dict_emploiterr[\"entreprise\"].append(entreprise)\n",
    "        # On extrait le d√©partement de la collectivit√© qui recrute\n",
    "        departement = job.find(\"span\", class_=\"font-weight-bold text-secondary set-font-size-0-9em ml-2\")\n",
    "        if departement:\n",
    "            departement = departement.getText().replace(\"\\n\",\"\").strip()\n",
    "        else:\n",
    "            departement = \"Non sp√©cifi√©\"\n",
    "        dict_emploiterr[\"departement\"].append(departement)\n",
    "\n",
    "        # On extrait le lien de l'offre d'emploi et on lui ajoute le pr√©fixe pour √™tre complet\n",
    "        link = job.find(\"a\", href=True)[\"href\"]\n",
    "        dict_emploiterr[\"link\"].append(\"https://www.emploi-territorial.fr\" + link)\n",
    "\n",
    "        # On extrait la date de publication\n",
    "        publication = job.find(\"button\", class_=\"btn btn-icon col text-center\")[\"data-tooltip\"].split(\"publi√© le \")[1]\n",
    "        date = datetime.strptime(publication, '%d/%m/%Y')\n",
    "        dict_emploiterr[\"date_publication\"].append(date)\n",
    "\n",
    "        dict_emploiterr[\"type_contrat\"].append(\"Fonction Publique\")\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_emploiterr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant d'aller directement sur chaque offre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour chaque lien de chaque offre,\n",
    "for link in dict_emploiterr[\"link\"]:\n",
    "    try:\n",
    "        print(link)\n",
    "        # On acc√®de √† l'URL de l'offre d'emploi\n",
    "        job_page = requests.get(link)\n",
    "        # On v√©rifie la r√©ussite de la connection\n",
    "        job_page.raise_for_status()\n",
    "\n",
    "        # On transforme cela en objet BeautifulSoup\n",
    "        soup2 = BeautifulSoup(job_page.content, \"html.parser\")\n",
    "        \n",
    "        # On cr√©e une liste vide de paragraphes\n",
    "        liste_paragraphe = []\n",
    "        # On extrait les diff√©rentes parties contenant le texte de l'offre\n",
    "        texte = soup2.find_all(\"div\", class_=\"offre-item-text row px-3\")\n",
    "        # Pour chaque carr√© de texte,\n",
    "        for paragraphes in texte: \n",
    "            # On r√©cup√®re le texte et on le nettoie\n",
    "            par_clean = paragraphes.getText().replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\'\", \"'\").strip()\n",
    "            par_spaceout = re.sub(r'\\s+',\" \",par_clean)\n",
    "            # On l'ajoute √† une liste de paragraphes\n",
    "            liste_paragraphe.append(par_spaceout)\n",
    "        # On ajoute les diff√©rentes paragraphes aux diff√©rentes listes du dictionnaire\n",
    "        dict_emploiterr[\"descriptif\"].append(liste_paragraphe[0] + liste_paragraphe[1])\n",
    "        dict_emploiterr[\"profil\"].append(liste_paragraphe[2])\n",
    "        \n",
    "        # On extrait la ville et le pays de l'entreprise\n",
    "        adresse = soup2.find(\"div\", class_=\"offre-item-value col-8\").getText().split()\n",
    "        ville = adresse[len(adresse)-1]\n",
    "        dict_emploiterr[\"ville\"].append(ville.capitalize())\n",
    "        dict_emploiterr[\"pays\"].append(\"France\")\n",
    "\n",
    "\n",
    "        # On applique un d√©lai al√©atoire de 1 √† 5 secondes entre chaque page pour ne pas surcharger les serveurs\n",
    "        time.sleep(random.uniform(1, 5))\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emploiterr = pd.DataFrame(dict_emploiterr)\n",
    "df_emploiterr[\"salaire\"] = \"Non sp√©cifi√©\"\n",
    "df_emploiterr[\"experience\"] = \"Non sp√©cifi√©\"\n",
    "df_emploiterr[\"source\"] = 'Emploi-Territorial'\n",
    "df_emploiterr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On extrait le fichier csv des offres r√©cup√©r√©es.\n",
    "df_emploiterr.to_csv(\"empterritorial_10122023.csv\", index=None, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobs that make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour r√©cup√©rer les informations pertinentes\n",
    "dict_makesense = {\n",
    "    \"entreprise\": [],\n",
    "    \"titre\": [],\n",
    "    \"type_contrat\": [],\n",
    "    \"ville\": [],\n",
    "    \"pays\": [],\n",
    "    \"link\": [],\n",
    "    \"date_publication\": [],\n",
    "    \"descriptif\": [],\n",
    "    \"profil\": [], \n",
    "    \"salaire\" : [],\n",
    "    \"experience\": [],\n",
    "    \"source\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cr√©e une liste des diff√©rentes pages\n",
    "URL_list = []\n",
    "\n",
    "# On cr√©e l'URL pour chaque page\n",
    "for n in range(0,5): # On commence √† partir de z√©ro pour respecter le format du site de JobsMakeSense\n",
    "    if n == 0 :\n",
    "        URL_list.append(\"https://jobs.makesense.org/fr/s/jobs/all?s=data&sortBy=relevance\")\n",
    "    URL_list.append(f'https://jobs.makesense.org/fr/s/jobs/all?s=data&sortBy=relevance&items_page={n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On int√©ragit tout d'abord avec la page principale de recherche. Voici un exemple avec juste la page 1 (recherche du 11/01/2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pages in URL_list: \n",
    "    try:\n",
    "        print(pages)\n",
    "        page = requests.get(pages)\n",
    "        # V√©rifie que la requ√™te a √©t√© correctement r√©alis√©e\n",
    "        page.raise_for_status()  \n",
    "\n",
    "        # Cr√©e un objet BeautifulSoup repr√©sentant le document HTML\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        # On extrait la partie de la page qui contient les offres\n",
    "        results = soup.find(class_=\"componentList\")\n",
    "\n",
    "        # On r√©cup√®re tous les √©l√©ments contenant \"aria-label\", c√†d chaque offre.\n",
    "        job_liste = results.find_all(\"div\", {\"aria-label\": True})\n",
    "        # Pour chaque \"carte\" d'offre d'emploisur la page de requ√™te\n",
    "        for job in job_liste:\n",
    "            # On extrait le lien de l'offre d'emploi et on lui ajoute le pr√©fixe pour √™tre complet\n",
    "            link = job.find(\"a\", href=True)[\"href\"]\n",
    "            if \"programs\" not in link:\n",
    "                dict_makesense[\"link\"].append(\"https://jobs.makesense.org\" + link)\n",
    "                # On extrait le titre de l'offre d'emploi et on nettoie le texte\n",
    "                title = job.find(\"h3\", class_=\"content__title\").getText().replace(\"\\n\",\"\").strip()\n",
    "                dict_makesense[\"titre\"].append(title)\n",
    "                \n",
    "                # On extrait les m√©ta-informations de la carte\n",
    "                details = job.find(\"div\", class_=\"job__meta job__metas\").getText()\n",
    "\n",
    "                # On nettoie la chaine de caract√®re\n",
    "                details = details.replace(\"üí°\", \"\")\n",
    "                # Les mots sont s√©par√©s par des s√©ries d'espaces, on s√©pare les diff√©rents items\n",
    "                details = re.split(r'\\s{2,}', details)\n",
    "                # Sur ce site, le nom de l'entreprise occupe la premi√®re place\n",
    "                if details[1]:\n",
    "                    dict_makesense[\"entreprise\"].append(details[1])\n",
    "                # S'il est not√©, le type de contrat occupe la troisi√®me place\n",
    "                if len(details) > 2:\n",
    "                    dict_makesense[\"type_contrat\"].append(details[3])\n",
    "\n",
    "        # On applique un d√©lai al√©atoire de 1 √† 3 secondes entre chaque page pour ne pas surcharger les serveurs\n",
    "        time.sleep(random.uniform(1, 3)) \n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_makesense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_makesense[\"link\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_makesense[\"entreprise\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_makesense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour chaque lien de chaque offre,\n",
    "for link in dict_makesense[\"link\"]:\n",
    "    if \"program/\" not in link:\n",
    "        try:\n",
    "            print(link)\n",
    "            # On acc√®de √† l'URL de l'offre d'emploi\n",
    "            job_page = requests.get(link)\n",
    "            # On v√©rifie la r√©ussite de la connection\n",
    "            job_page.raise_for_status()\n",
    "\n",
    "            # On transforme cela en objet BeautifulSoup\n",
    "            soup2 = BeautifulSoup(job_page.content, \"html.parser\")\n",
    "            \n",
    "            # On extrait la description de l'offre\n",
    "            dict_makesense[\"descriptif\"].append(soup2.find(\"main\", class_=\"job__main-content\").getText().replace(\"\\n\", \" \"))\n",
    "            # On extrait le profil recherch√©\n",
    "            try:\n",
    "                dict_makesense[\"profil\"].append(soup2.find(\"div\", class_=\"job__main-content\").getText().replace(\"\\n\", \" \"))\n",
    "            except: \n",
    "                dict_makesense[\"profil\"].append(\"Non sp√©cifi√©\")\n",
    "            # On extrait la ville et le pays de l'entreprise\n",
    "            lieu = soup2.find(\"address\").getText().replace(\"\\n\", \" \").strip()\n",
    "            if \",\" in lieu:\n",
    "                split_lieu = lieu.split(\", \")\n",
    "                if len(split_lieu) == 2:\n",
    "                    ville, pays = split_lieu\n",
    "                    dict_makesense[\"ville\"].append(ville)\n",
    "                    dict_makesense[\"pays\"].append(pays)\n",
    "                else:\n",
    "                    dict_makesense[\"ville\"].append(lieu)\n",
    "                    dict_makesense[\"pays\"].append(\"Non Sp√©cifi√©\")                   \n",
    "            else: \n",
    "                dict_makesense[\"ville\"].append(lieu)\n",
    "                dict_makesense[\"pays\"].append(\"Non Sp√©cifi√©\")\n",
    "\n",
    "            # On extrait la date de publication et on ne garde que la date en format dd/mm/yyyy\n",
    "            if \"Publi√©\" in soup2.get_text():\n",
    "                publication = soup2.find(text=lambda text: \"Publi√©e\" in text).replace(\"\\n\", '').strip().split(\"Publi√©e le \")[1]\n",
    "                date = datetime.strptime(publication, '%d/%m/%Y')\n",
    "                dict_makesense[\"date_publication\"].append(date)\n",
    "            else: \n",
    "                dict_makesense[\"date_publication\"].append(\"Non sp√©cifi√©\")\n",
    "            dict_makesense[\"salaire\"].append(\"Non sp√©cifi√©\")\n",
    "            dict_makesense[\"experience\"].append(\"Non sp√©cifi√©\")\n",
    "            dict_makesense[\"source\"].append(\"JobsThatMakeSense\")\n",
    "\n",
    "            # On applique un d√©lai al√©atoire de 1 √† 10 secondes entre chaque page pour ne pas surcharger les serveurs\n",
    "            time.sleep(random.uniform(1, 10))\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict_makesense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict_makesense).to_csv(\"makesense_11012024.csv\", index=None, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
